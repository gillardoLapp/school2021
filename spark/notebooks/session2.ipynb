{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ESCAPE Summer School 2021: Big data for big science #2\n",
    "\n",
    "<img src=\"../pictures/spark_escape_logo.png\" alt=\"alt text\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Context\n",
    "\n",
    "In this second notebook, we will learn on concrete examples how to interface and play with popular scientific libraries (Numpy, Pandas, ...).\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Interfacing popular Python scientific libraries with Apache Spark\n",
    "- Developing your own modules for Spark\n",
    "- Inspect, test, and debug Spark programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment these lines if you are using Google Colab\n",
    "# !pip install pyspark==3.1.1\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialise our Spark session\n",
    "# spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Through this series of exercises, we will use the same dataset as in the first session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data into a Spark DataFrame\n",
    "df = spark.read.format(\"parquet\").load(\"../data/clusters.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## User defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark has many built-in functions, but it is often limited for scientific purposes. Ideally, you would like also to be able to apply any complex logic to your data. This is done through `User-Defined Functions` (UDFs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simple UDF\n",
    "\n",
    "UDFs do not modify columns directly (concept of _immutability_) - you will create a new column in the DataFrame instead and populate it with your logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def add_one(element):\n",
    "    return element + 1\n",
    "\n",
    "add_one_udf = udf(add_one)\n",
    "\n",
    "df.withColumn('idPlusOne', add_one_udf(df['id'])).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font color=red> \n",
    " Exercise 1 : \n",
    " Define an UDF to compute the sum of x,y,z\n",
    "</font>\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Limitations\n",
    "\n",
    "1. UDFs let you to define any processing to be done on the data, but they come at a price: they are black boxes for Spark! This means you will not benefit from the built-in optimisations that Spark offers.\n",
    "2. For each element of the DataFrame, the UDF is called - hence it can be super slow!\n",
    "\n",
    "Always prefer a built-in Spark function rather than a UDF if the alternative exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit a = df.withColumn('idPlusOne', add_one_udf(df['id'])).collect()\n",
    "%timeit b = df.withColumn('idPlusOne', df['id'] + 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Speeding-up UDF: Pandas UDF\n",
    "\n",
    "To overcome the inefficiency of UDFs, you can use instead `Pandas UDFs`. They can be seen as vectorised UDFs. They use Pandas Series and pyarrow under the hood to speed-up the data movement and computation.\n",
    "\n",
    "<img src=\"../pictures/spark_udf.png\" alt=\"alt text\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simple Pandas UDF\n",
    "\n",
    "Pandas UDF are decorated functions. They take as input one (or several) column of the DataFrame, and output Pandas Series. Our basic UDF can be rewritten as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf('int')\n",
    "def add_one_pandas(colAsSeries: pd.Series) -> pd.Series:\n",
    "    return colAsSeries + 1\n",
    "\n",
    "df.withColumn('idPlusOne', add_one_pandas(df['id'])).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit a = df.withColumn('idPlusOne', add_one_udf(df['id'])).collect()\n",
    "%timeit b = df.withColumn('idPlusOne', df['id'] + 1).collect()\n",
    "%timeit c = df.withColumn('idPlusOnePandas', add_one_pandas(df['id'])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pandas UDF types\n",
    "\n",
    "As of Spark 3.1, there are several Pandas UDF types:\n",
    "1. Series to Series (and Iterator of Series to Iterator of Series)\n",
    "2. Series to Scalar\n",
    "3. Map\n",
    "4. Grouped Map (and Co-grouped Map)\n",
    "\n",
    "See this [post](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Series to Series\n",
    "\n",
    "This corresponds to the previous example. It expects the given function to take one or more `pandas.Series` and outputs one `pandas.Series`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../pictures/pudf_1.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf('int')\n",
    "def add_one_pandas(colAsSeries: pd.Series) -> pd.Series:\n",
    "    return colAsSeries + 1\n",
    "\n",
    "df.withColumn('idPlusOne', add_one_pandas(df['id'])).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Series to Scalar\n",
    "\n",
    "The function takes one or more `pandas.Series` and outputs a primitive data type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../pictures/pudf_2.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import Window\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def extract_mean(col: pd.Series) -> float:\n",
    "    return col.mean()\n",
    "\n",
    "# Compute the mean over all data\n",
    "df.select(extract_mean(df['x']).alias('meanx_total')).show()\n",
    "\n",
    "# Compute the mean per ID group\n",
    "df.groupby(\"id\").agg(extract_mean(df['x']).alias('meanx_per_group')).show()\n",
    "\n",
    "# Compute the mean per ID group, and reassign it back to all elements\n",
    "df.withColumn('meanx_per_group', extract_mean(df['x']).over(Window.partitionBy('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Map\n",
    "\n",
    "It maps every batch in each partition and transforms each. The function takes an iterator of `pandas.DataFrame` and outputs an iterator of `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../pictures/pudf_3.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "def pandas_filter(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]\n",
    "\n",
    "# Extract only rows with ID=1\n",
    "df.mapInPandas(pandas_filter, schema=df.schema).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grouped Map (experimental API)\n",
    "\n",
    "Grouped map in the Pandas Function API is `applyInPandas` at a grouped DataFrame, e.g., `df.groupby(...)`. It maps each group to each `pandas.DataFrame` in the function. Note that it does not require for the output to be the same length of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"../pictures/pudf_4.png\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def subtract_x_mean(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    x = pdf.x\n",
    "    return pdf.assign(x=x - x.mean())\n",
    "\n",
    "df.select(['x', 'id']).groupby(\"id\").applyInPandas(subtract_x_mean, schema='x double, id long').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "**Exercise:** Use a pandas UDF to compute the distance of each row to the center (x, y, z) = (0, 0, 0), and store the result in a new Dataframe column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf('double')\n",
    "def compute_distance_to_center(x, y, z):\n",
    "    \"\"\" Compute the distance to the center (0, 0, 0)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y, z: double\n",
    "        row coordinates\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    series: pandas Series\n",
    "        Series containing distance to the center for each row\n",
    "    \"\"\"\n",
    "    r_square = x*x + y*y + z*z\n",
    "    return pd.Series(np.sqrt(r_square))\n",
    "\n",
    "df.withColumn(\n",
    "    \"distance\", \n",
    "    compute_distance_to_center(\n",
    "        df[\"x\"],\n",
    "        df[\"y\"],\n",
    "        df[\"z\"]\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise:** As in session 1, find the barycentre of each clusters in the dataset but this time using aggregation and user defined function (hint: look for `GROUPED MAP`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "def compute_barycentre(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Compute the barycentre of a partition\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf : pandas DataFrame\n",
    "        pandas DataFrame containing partition data\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    Pandas DataFrame with barycentre coordinates.\n",
    "    \"\"\"\n",
    "    mean = pdf.mean()\n",
    "\n",
    "    out = {colname:[value] for colname, value in zip(mean.keys(), mean.values)}\n",
    "    \n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "df.groupBy(\"id\").applyInPandas(compute_barycentre, schema=df.schema).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Debugging Spark application: Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finding the root of a problem in a distributed environment is not easy: the logs are usually on the executors (or redirected to an external storage system). The Spark UI is a tool that helps you visualising the processing, resources utilisation, and accessing the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's start the history server\n",
    "!. $SPARK_HOME/sbin/start-history-server.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark UI\n",
    "\n",
    "- It contains the details of the current job. Automatically launched when the job starts, the Spark UI closes at the end of the job. The default port is 4040: http://127.0.0.1:4040\n",
    "\n",
    "### History Server\n",
    "\n",
    "- The history server contains all information about all the previous jobs. You need to start it manually. The default port is 18080: http://127.0.0.1:18080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Testing Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As any tools, you must test your code: unit tests, integration tests, etc. They all apply for Spark. Exit the notebook (save your running notebook, shutdown the kernels, and CTRL+C the terminal tab where Jupyter has been launched), and enter the container (see `launch_container.sh`) to practice a bit:\n",
    "\n",
    "```bash\n",
    "# in school2021/spark\n",
    "docker run -it --rm  \\\n",
    "    -v $PWD:/home/jovyan/work:rw \\\n",
    "    -p 8888:8888 -p 4040:4040 -p 18080:18080 \\\n",
    "    spark_escape2021 bash \\\n",
    "    \n",
    "# You should see a similar prompt\n",
    "(base) jovyan@77081e01d859:~/work$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "<img src=\"../pictures/logo-Escape_0.png\" alt=\"alt text\" width=\"400\" align=\"right\"/>\n",
    "\n",
    "This event is organized in the framework and with the support of the European Science Cluster of Astronomy & Particle physics ESFRI research infrastructures (ESCAPE), funded by the European Union's Horizon 2020 - Grant N. 824064."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
